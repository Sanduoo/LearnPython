{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 反向传播的目的：优化模型参数\n",
    "# 损失函数（loss）：预测值（y）与已知答案（y_）的差距\n",
    "#         均方误差（MSE）loss = tf.reduce_mean(tf.square(y_-y))\n",
    "# 反向传播训练方法：以减小loss值为优化目标\n",
    "#         tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)梯度下降\n",
    "#             　　　　标准梯度下降法(GD, Gradient Descent)\n",
    "#                         基本策略可以理解为”在有限视距内寻找最快路径下山“，因此每走一步，参考当前位置最陡的方向(即梯度)进而迈出下一步\n",
    "#                         评价：标准梯度下降法主要有两个缺点:\n",
    "#                                 训练速度慢：\n",
    "#                                 每走一步都要要计算调整下一步的方向，下山的速度变慢\n",
    "# 　　　　　　　　                在应用于大型数据集中，每输入一个样本都要更新一次参数，且每次迭代都要遍历所有的样本\n",
    "# 　　　　　　　　                会使得训练过程及其缓慢，需要花费很长时间才能得到收敛解\n",
    "#                               容易陷入局部最优解：\n",
    "# 　　　　　　　　                由于是在有限视距内寻找下山的反向\n",
    "# 　　　　　　　　                当陷入平坦的洼地，会误以为到达了山地的最低点，从而不会继续往下走\n",
    "# 　　　　　　　　                所谓的局部最优解就是鞍点。落入鞍点，梯度为0，使得模型参数不在继续更新\n",
    "                        \n",
    "#                     随机梯度下降法(SGD, Stochastic Gradient Descent)\n",
    "#                         基本策略可以理解为，在下山之前掌握了附近的地势情况，选择总体平均梯度最小的方向下山\n",
    "#                         评价：\n",
    "#                                 批量梯度下降法比标准梯度下降法训练时间短，且每次下降的方向都很正确\n",
    "                            \n",
    "#                     批量梯度下降法(BGD, Batch Gradient Descent)\n",
    "#                         基本策略可以理解为随机梯度下降像是一个盲人下山，不用每走一步计算一次梯度，但是他总能下到山底，只不过过程会显得扭扭曲曲\n",
    "#                         评价：\n",
    "#                           优点：\n",
    "#                                 虽然SGD需要走很多步的样子，但是对梯度的要求很低（计算梯度快）。而对于引入噪声，大量的理论和实践工作证明，只要噪声不是特别大，SGD都能很好地收敛。\n",
    "#                                 应用大型数据集时，训练速度很快。比如每次从百万数据样本中，取几百个数据点，算一个SGD梯度，更新一下模型参数。\n",
    "#                                 相比于标准梯度下降法的遍历全部样本，每输入一个样本更新一次参数，要快得多。\n",
    "#                           缺点：\n",
    "#                                 SGD在随机选择梯度的同时会引入噪声，使得权值更新的方向不一定正确。\n",
    "#                                 此外，SGD也没能单独克服局部最优解的问题。\n",
    "#                                 由于这种方法是在一次更新中，就对整个数据集计算梯度，\n",
    "#                                 所以计算起来非常慢，遇到很大量的数据集也会非常棘手，而且不能投入新数据实时更新模型\n",
    "                                \n",
    "#         tf.train.MomentumOptimizer(learning_rate).minimize(loss)\n",
    "#                 标准动量优化方法Momentum\n",
    "#                 理解策略为：由于当前权值的改变会受到上一次权值改变的影响，类似于小球向下滚动的时候带上了惯性。\n",
    "#                             这样可以加快小球向下滚动的速度。\n",
    "                \n",
    "#                 牛顿加速梯度（NAG, Nesterov accelerated gradient）算法\n",
    "#                 理解策略：在Momentun中小球会盲目地跟从下坡的梯度，容易发生错误。\n",
    "#                           所以需要一个更聪明的小球，能提前知道它要去哪里，还要知道走到坡底的时候速度慢下来而不是又冲上另一个坡。\n",
    "                    \n",
    "#         tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "#                 自适应矩估计（Adaptive Moment Estimation）\n",
    "#                 概率论中矩的含义是：如果一个随机变量 X 服从某个分布，X 的一阶矩是 E(X)，也就是样本平均值，X 的二阶矩就是 E(X^2)，\n",
    "#                                     也就是样本平方的平均值。\n",
    "#                 Adam 算法利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。\n",
    "#                 评价：\n",
    "#                         相比于SGD算法：\n",
    "#                                         不容易陷于局部优点\n",
    "#                                         速度更快，学习效果更为有效\n",
    "#                                         纠正其他优化技术中存在的问题，如学习率消失或是高方差的参数更新导致损失函数波动较大等问题。\n",
    "#                                         Adam 的调参相对简单，默认参数就可以处理绝大部分的问题。\n",
    "                                    \n",
    "#         学习率：每次参数更新的幅度\n",
    "    \n",
    "#导入模块，生成模拟数据集\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "BATCH_SIZE = 4   #一次喂入神经网络8组数据\n",
    "seed = 23455     #随机种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#基于seed产生随机数\n",
    "rng = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#随机数返回32行2列的矩阵 表示32组 体积和重量 作为输入数据集\n",
    "X = rng.rand(32,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      " [[0.83494319 0.11482951]\n",
      " [0.66899751 0.46594987]\n",
      " [0.60181666 0.58838408]\n",
      " [0.31836656 0.20502072]\n",
      " [0.87043944 0.02679395]\n",
      " [0.41539811 0.43938369]\n",
      " [0.68635684 0.24833404]\n",
      " [0.97315228 0.68541849]\n",
      " [0.03081617 0.89479913]\n",
      " [0.24665715 0.28584862]\n",
      " [0.31375667 0.47718349]\n",
      " [0.56689254 0.77079148]\n",
      " [0.7321604  0.35828963]\n",
      " [0.15724842 0.94294584]\n",
      " [0.34933722 0.84634483]\n",
      " [0.50304053 0.81299619]\n",
      " [0.23869886 0.9895604 ]\n",
      " [0.4636501  0.32531094]\n",
      " [0.36510487 0.97365522]\n",
      " [0.73350238 0.83833013]\n",
      " [0.61810158 0.12580353]\n",
      " [0.59274817 0.18779828]\n",
      " [0.87150299 0.34679501]\n",
      " [0.25883219 0.50002932]\n",
      " [0.75690948 0.83429824]\n",
      " [0.29316649 0.05646578]\n",
      " [0.10409134 0.88235166]\n",
      " [0.06727785 0.57784761]\n",
      " [0.38492705 0.48384792]\n",
      " [0.69234428 0.19687348]\n",
      " [0.42783492 0.73416985]\n",
      " [0.09696069 0.04883936]]\n",
      "Y:\n",
      " [[1], [0], [0], [1], [1], [1], [1], [0], [1], [1], [1], [0], [0], [0], [0], [0], [0], [1], [0], [0], [1], [1], [0], [1], [0], [1], [1], [1], [1], [1], [0], [1]]\n"
     ]
    }
   ],
   "source": [
    "Y = [[int(x0 + x1<1)] for (x0,x1) in X]\n",
    "print(\"X:\\n\",X)\n",
    "print(\"Y:\\n\",Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义NN输入、参数、输出\n",
    "x = tf.placeholder(tf.float32,shape=(None,2))\n",
    "y_ = tf.placeholder(tf.float32,shape=(None,1))\n",
    "w1 = tf.Variable(tf.random.normal([2,3],stddev=1,seed=1))\n",
    "w2 = tf.Variable(tf.random.normal([3,1],stddev=1,seed=1))\n",
    "a = tf.matmul(x,w1)\n",
    "y = tf.matmul(a,w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义损失函数及反向传播方法\n",
    "loss = tf.reduce_mean(tf.square(y-y_))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
    "#train_step = tf.train.MomentumOptimizer(0.001).minimize(loss)\n",
    "#train_step = tf.train.AdamOptimizer(0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########未经训练w1、w2############\n",
      "w1:\n",
      " [[-0.8113182   1.4845988   0.06532937]\n",
      " [-2.4427042   0.0992484   0.5912243 ]]\n",
      "w2:\n",
      " [[-0.8113182 ]\n",
      " [ 1.4845988 ]\n",
      " [ 0.06532937]]\n",
      "After 0 traning step(s),loss on all data is 5.14196\n",
      "After 500 traning step(s),loss on all data is 0.428969\n",
      "After 1000 traning step(s),loss on all data is 0.409785\n",
      "After 1500 traning step(s),loss on all data is 0.399936\n",
      "After 2000 traning step(s),loss on all data is 0.394156\n",
      "After 2500 traning step(s),loss on all data is 0.390617\n",
      "After 3000 traning step(s),loss on all data is 0.388349\n",
      "After 3500 traning step(s),loss on all data is 0.386875\n",
      "After 4000 traning step(s),loss on all data is 0.385874\n",
      "After 4500 traning step(s),loss on all data is 0.385205\n",
      "After 5000 traning step(s),loss on all data is 0.384728\n",
      "After 5500 traning step(s),loss on all data is 0.384408\n",
      "After 6000 traning step(s),loss on all data is 0.384167\n",
      "After 6500 traning step(s),loss on all data is 0.38401\n",
      "After 7000 traning step(s),loss on all data is 0.383883\n",
      "After 7500 traning step(s),loss on all data is 0.383805\n",
      "After 8000 traning step(s),loss on all data is 0.383734\n",
      "After 8500 traning step(s),loss on all data is 0.383696\n",
      "After 9000 traning step(s),loss on all data is 0.383655\n",
      "After 9500 traning step(s),loss on all data is 0.383639\n",
      "w1:\n",
      " [[-0.6910346   0.81657857  0.09607729]\n",
      " [-2.3421273  -0.10762475  0.5851883 ]]\n",
      "w2:\n",
      " [[-0.08718564]\n",
      " [ 0.81653804]\n",
      " [-0.05228706]]\n"
     ]
    }
   ],
   "source": [
    "#生成会话，训练steps轮\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    print(\"##########未经训练w1、w2############\")\n",
    "    print(\"w1:\\n\",sess.run(w1))\n",
    "    print(\"w2:\\n\",sess.run(w2))\n",
    "    \n",
    "    #训练模型\n",
    "    STEPS = 10000\n",
    "    for i in range(STEPS):\n",
    "        start = (i*BATCH_SIZE) % 32\n",
    "        end = start + BATCH_SIZE\n",
    "        sess.run(train_step,feed_dict={x:X[start:end],y_: Y[start:end]})\n",
    "        if i % 500 == 0:\n",
    "            total_loss = sess.run(loss,feed_dict={x: X,y_ : Y})\n",
    "            print(\"After %d traning step(s),loss on all data is %g\"%(i,total_loss))\n",
    "        \n",
    "    #输出训练后的参数值\n",
    "    print(\"w1:\\n\",sess.run(w1))\n",
    "    print(\"w2:\\n\",sess.run(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
