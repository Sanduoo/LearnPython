{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学习率learning_rate:每次参数更新的幅度\n",
    "#     Wn+1 = Wn - learning_rate▽      ▽：损失函数loss的梯度（倒数）\n",
    "\n",
    "# eg：损失函数 loss=（w + 1）^2    梯度▽=（dloss）/（dw）=2w+2   (当w=-1时，▽=0，最优)\n",
    "# init_w=5,learning_rate=0.2\n",
    "# 1. w=5   5-0.2*（2*5+2）=2.6\n",
    "# 2. w=2.6   2.6-0.2*（2*2.6+2）=1.16\n",
    "# ......\n",
    "\n",
    "# 学习率大了会震荡不收敛，学习率小了会收敛速度慢\n",
    "# 这里引入指数衰减学习率这个概念：\n",
    "#     指数衰减学习率是先使用较大的学习率来快速得到一个较优的解，然后随着迭代的继续,逐步减小学习率，使得模型在训练后期更加稳定\n",
    "#     learning_rate=LEARNING_ATE_BASE*LEARNING_RATE_DECAY^(global_step/LEARNING_RATE_STEP)     LEARNING_RATE_STEP=总样本数/BATCH_SIZE\n",
    "#         学习率   =   学习率基数    *学习率衰减率（0，1）^(当前全局步数/多少轮更新一次学习率)    \n",
    "#     →global_step = tf.Variable(0,trainable=False)\n",
    "#       learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE,global_step,LEARNING_RATE_STEP,LEARNING_RATE_DECAY,staircase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入模块，生成模拟数据集\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "#最初学习率\n",
    "LEARNING_RATE_BASE = 0.1 \n",
    "#学习率衰减率\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "#喂入多少轮BATCH_SIZE后，更新一次学习率，一般设置为：总样本数/BATCH_SIZE-每次选取样本数\n",
    "LEARNING_RATE_STEP = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#运行轮数计数器\n",
    "global_step = tf.Variable(0,trainable=False)\n",
    "#定义指数下降学习率\n",
    "learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE,global_step,LEARNING_RATE_STEP,LEARNING_RATE_DECAY,staircase=True)\n",
    "#定义待优化参数\n",
    "w = tf.Variable(tf.constant(5,dtype=tf.float32))\n",
    "#定义损失函数loss\n",
    "loss = tf.square(w+1)\n",
    "#定义反向传播方法\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 steps: global_step is 1.000000,w is 3.800000,learning_rate is 0.099000,loss is 23.040001\n",
      "After 1 steps: global_step is 2.000000,w is 2.849600,learning_rate is 0.098010,loss is 14.819419\n",
      "After 2 steps: global_step is 3.000000,w is 2.095001,learning_rate is 0.097030,loss is 9.579033\n",
      "After 3 steps: global_step is 4.000000,w is 1.494386,learning_rate is 0.096060,loss is 6.221961\n",
      "After 4 steps: global_step is 5.000000,w is 1.015167,learning_rate is 0.095099,loss is 4.060896\n",
      "After 5 steps: global_step is 6.000000,w is 0.631886,learning_rate is 0.094148,loss is 2.663051\n",
      "After 6 steps: global_step is 7.000000,w is 0.324608,learning_rate is 0.093207,loss is 1.754587\n",
      "After 7 steps: global_step is 8.000000,w is 0.077684,learning_rate is 0.092274,loss is 1.161403\n",
      "After 8 steps: global_step is 9.000000,w is -0.121202,learning_rate is 0.091352,loss is 0.772287\n",
      "After 9 steps: global_step is 10.000000,w is -0.281761,learning_rate is 0.090438,loss is 0.515867\n",
      "After 10 steps: global_step is 11.000000,w is -0.411674,learning_rate is 0.089534,loss is 0.346128\n",
      "After 11 steps: global_step is 12.000000,w is -0.517024,learning_rate is 0.088638,loss is 0.233266\n",
      "After 12 steps: global_step is 13.000000,w is -0.602644,learning_rate is 0.087752,loss is 0.157891\n",
      "After 13 steps: global_step is 14.000000,w is -0.672382,learning_rate is 0.086875,loss is 0.107334\n",
      "After 14 steps: global_step is 15.000000,w is -0.729305,learning_rate is 0.086006,loss is 0.073276\n",
      "After 15 steps: global_step is 16.000000,w is -0.775868,learning_rate is 0.085146,loss is 0.050235\n",
      "After 16 steps: global_step is 17.000000,w is -0.814036,learning_rate is 0.084294,loss is 0.034583\n",
      "After 17 steps: global_step is 18.000000,w is -0.845387,learning_rate is 0.083451,loss is 0.023905\n",
      "After 18 steps: global_step is 19.000000,w is -0.871193,learning_rate is 0.082617,loss is 0.016591\n",
      "After 19 steps: global_step is 20.000000,w is -0.892476,learning_rate is 0.081791,loss is 0.011561\n",
      "After 20 steps: global_step is 21.000000,w is -0.910065,learning_rate is 0.080973,loss is 0.008088\n",
      "After 21 steps: global_step is 22.000000,w is -0.924629,learning_rate is 0.080163,loss is 0.005681\n",
      "After 22 steps: global_step is 23.000000,w is -0.936713,learning_rate is 0.079361,loss is 0.004005\n",
      "After 23 steps: global_step is 24.000000,w is -0.946758,learning_rate is 0.078568,loss is 0.002835\n",
      "After 24 steps: global_step is 25.000000,w is -0.955125,learning_rate is 0.077782,loss is 0.002014\n",
      "After 25 steps: global_step is 26.000000,w is -0.962106,learning_rate is 0.077004,loss is 0.001436\n",
      "After 26 steps: global_step is 27.000000,w is -0.967942,learning_rate is 0.076234,loss is 0.001028\n",
      "After 27 steps: global_step is 28.000000,w is -0.972830,learning_rate is 0.075472,loss is 0.000738\n",
      "After 28 steps: global_step is 29.000000,w is -0.976931,learning_rate is 0.074717,loss is 0.000532\n",
      "After 29 steps: global_step is 30.000000,w is -0.980378,learning_rate is 0.073970,loss is 0.000385\n",
      "After 30 steps: global_step is 31.000000,w is -0.983281,learning_rate is 0.073230,loss is 0.000280\n",
      "After 31 steps: global_step is 32.000000,w is -0.985730,learning_rate is 0.072498,loss is 0.000204\n",
      "After 32 steps: global_step is 33.000000,w is -0.987799,learning_rate is 0.071773,loss is 0.000149\n",
      "After 33 steps: global_step is 34.000000,w is -0.989550,learning_rate is 0.071055,loss is 0.000109\n",
      "After 34 steps: global_step is 35.000000,w is -0.991035,learning_rate is 0.070345,loss is 0.000080\n",
      "After 35 steps: global_step is 36.000000,w is -0.992297,learning_rate is 0.069641,loss is 0.000059\n",
      "After 36 steps: global_step is 37.000000,w is -0.993369,learning_rate is 0.068945,loss is 0.000044\n",
      "After 37 steps: global_step is 38.000000,w is -0.994284,learning_rate is 0.068255,loss is 0.000033\n",
      "After 38 steps: global_step is 39.000000,w is -0.995064,learning_rate is 0.067573,loss is 0.000024\n",
      "After 39 steps: global_step is 40.000000,w is -0.995731,learning_rate is 0.066897,loss is 0.000018\n"
     ]
    }
   ],
   "source": [
    "#生成会话，训练steps轮\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "   \n",
    "    #训练模型\n",
    "    STEPS = 40\n",
    "    for i in range(STEPS):\n",
    "        sess.run(train_step)\n",
    "        learning_rate_val = sess.run(learning_rate)\n",
    "        global_step_val = sess.run(global_step)\n",
    "        w_val = sess.run(w)\n",
    "        loss_val = sess.run(loss)\n",
    "        print(\"After %s steps: global_step is %f,w is %f,learning_rate is %f,loss is %f\"%(i,global_step_val,w_val,learning_rate_val,loss_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
