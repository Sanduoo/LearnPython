{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN复杂度\n",
    "## 空间复杂度\n",
    "层数 = 隐藏层数 + 1个输出层  \n",
    "以3-4-2为例：一个隐藏层+一个输出层=2层  \n",
    "总参数 = 总w + 总b  \n",
    "以3-4-2为例：3*4+4 + 4*2+2=26  \n",
    "## 时间复杂度\n",
    "乘加运算的次数  \n",
    "以3-4-2为例：3*4+4*2=20  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学习率\n",
    "$$ w_{t+1} = w_t - lr*\\frac{\\partial loss}{\\partial w_t} $$\n",
    "## 指数衰减学习率\n",
    "$$ 指数衰减学习率 = 初始学习率 * 学习率衰减率^{当前轮数 / 多少轮衰减一次} $$\n",
    "eg：损失函数 loss=（w + 1）^2 | 梯度▽=（dloss）/（dw）=2w+2 | (当w=-1时，▽=0，最优)  \n",
    "init_w=5,learning_rate=0.2  \n",
    "1. | w=5 | 5-0.2*（2*5+2）=2.6  \n",
    "2. | w=2.6 | 2.6-0.2*（2*2.6+2）=1.16  \n",
    "......  \n",
    "学习率大了会震荡不收敛，学习率小了会收敛速度慢  \n",
    "这里引入指数衰减学习率这个概念：  \n",
    "指数衰减学习率是先使用较大的学习率来快速得到一个较优的解，然后随着迭代的继续,逐步减小学习率，使得模型在训练后期更加稳定  \n",
    "eg: lr = LR_BASE * LR_DECAY ** (epoch / LR_STEP)  \n",
    "非线性、可微性-优化器使用梯度下降、单调性-单层神经网络的损失函数是凸函数、近似恒等-f(x)≈x,当参数初始化为随机小值时，神经网络更稳定  \n",
    "激活函数输出为有限值时，基于梯度的优化方法更稳定  \n",
    "激活函数输出为无限值时，建议调小学习率  \n",
    "### Sigmoid函数\n",
    "$$ f(x) = \\frac{1}{1+e^{-x}} $$\n",
    "tf.nn.sigmoid(x)  \n",
    "#### 特点\n",
    "易造成梯度消失、输出非0均值，收敛慢、幂运算复杂度，训练时间长  \n",
    "### TanH函数\n",
    "$$ f(x) = \\frac{1-e^{-2x}}{1+e^{-2x}} $$\n",
    "tf.math.tanh(x)\n",
    "#### 特点\n",
    "输出的是0均值、易造成梯度消失、幂运算复杂度，训练时间长\n",
    "### Relu函数\n",
    "$$ f(x) = max(x,0)=\\begin{cases}0,&x<0 \\\\ x,&x>=0 \\end{cases} $$\n",
    "tf.nn.relu(x)\n",
    "#### 优点\n",
    "解决了梯度消失问题（在正区间）、只需判断输入是否大于0，计算速度快、收敛速度远快于sigmoid和tanh  \n",
    "#### 缺点\n",
    "输出非0均值，收敛慢、Dead RelU问题：某些神经元可能永远不被激活，导致相应的参数永远不能被更新（输入负数特征过多，减少学习率，减少参数分布的巨大变化）。\n",
    "### Leaky Relu函数\n",
    "$$ f(x) = max(αx,x)=\\begin{cases}αx,&x<0 \\\\ x,&x>=0 \\end{cases} $$\n",
    "tf.nn.leaky_relu(x)\n",
    "#### 特点\n",
    "理论上leaky_relu有relu的优点，但不会出现dead relu问题，实际上效果不一定优于relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips\n",
    "首选relu激活函数  \n",
    "学习率设置较小  \n",
    "输入特征标准化，即让输入特征满足以0为均值，1为标准差的正态分布  \n",
    "初始参数中心化，即让随机生成的参数满足以0为均值，$$ \\sqrt{\\frac{2}{当前层输入特征个数}} $$为标准差的正态分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
