{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1942年McCulloch Pitts 神经元模型\n",
    "# f（ΣXi*Wi+b）————f-激活函数activation function、b-偏置项bias\n",
    "# 常用的激活函数：\n",
    "#     ReLU f(x)=max(x,0)→tf.nn.relu()\n",
    "#     Sigmoid f(x)=1/(1+1/e^x)→tf.nn.sigmoid()         limf(+∞)=1，limf(-∞)=0，f(0)=0.5\n",
    "#     TanH f(x)=(1-1/(e^2))/(1+1/(e^2))→tf.nn.tanh()   limf(+∞)=1，limf(-∞)=-1，f(0)=0\n",
    "#     Swish f(x)=x*Sigmoid                              limf(+∞)=+∞，limf(-∞)=-0，f(0)=0\n",
    "# NN优化的目标：loss最小   均方误差MSE（Mean Squared Error），mse(y_,y)=(Σ(y-y_)^2)/n→loss_mse=tf.reduce_mean(tf.square(y_-y))\n",
    "#                          自定义,loss(y_,y)=Σf(y_,y)\n",
    "#                          交叉熵CE（Cross Entropy）,表征两个概率分布之间的距离,概率分布越远，交叉熵分布越小\n",
    "#                                                    H(y_,y)=-Σ(y_*logy)→ce=tf.reduce_mean(y_*tf.log(tf.clip_by_value(y,1e-12,1.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax：某元素的指数与所有元素指数和的比值\n",
    "# softmax用于多分类过程中，它将多个神经元的输出，映射到（0,1）区间内，可以看成概率来理解，从而来进行多分类\n",
    "# softmax(Yi)=(e^Yi)/(Σe^Yi)→ce=tf.sparse_softmax_cross_entropy_with_logits(logits=y,labels=tf.argmax(y_,1))\n",
    "#                             cem=tf.reduce_mean(ce)     当前预测值与标准答案的差距=loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入模块，生成模拟数据集\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "BATCH_SIZE = 4   #一次喂入神经网络8组数据\n",
    "seed = 23455     #随机种子\n",
    "COST = 9\n",
    "PROFIT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdm = np.random.RandomState(seed)\n",
    "X = rdm.rand(32,2)\n",
    "Y_ = [[x1+x2+(rdm.rand()/10.0-0.05)] for (x1,x2) in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义NN输入、参数、输出,定义前向传播\n",
    "x = tf.placeholder(tf.float32,shape=(None,2))\n",
    "y_ = tf.placeholder(tf.float32,shape=(None,1))\n",
    "w1 = tf.Variable(tf.random.normal([2,1],stddev=1,seed=1))\n",
    "y = tf.matmul(x,w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义损失函数及反向传播方法\n",
    "loss = tf.reduce_sum(tf.where(tf.greater(y,y_),(y-y_)*COST,(y_-y)*PROFIT))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
    "#train_step = tf.train.MomentumOptimizer(0.001).minimize(loss)\n",
    "#train_step = tf.train.AdamOptimizer(0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 traning step(s),loss on all data is 29.8645\n",
      "w1:\n",
      " [[-0.8088941]\n",
      " [ 1.4859729]]\n",
      "After 500 traning step(s),loss on all data is 14.341\n",
      "w1:\n",
      " [[0.02260804]\n",
      " [1.1564052 ]]\n",
      "After 1000 traning step(s),loss on all data is 2.39831\n",
      "w1:\n",
      " [[0.8723926]\n",
      " [1.0072129]]\n",
      "After 1500 traning step(s),loss on all data is 1.47386\n",
      "w1:\n",
      " [[0.95760536]\n",
      " [0.97100633]]\n",
      "After 2000 traning step(s),loss on all data is 1.4655\n",
      "w1:\n",
      " [[0.96072936]\n",
      " [0.9691097 ]]\n",
      "After 2500 traning step(s),loss on all data is 1.45911\n",
      "w1:\n",
      " [[0.9603584]\n",
      " [0.9706179]]\n",
      " final w1:\n",
      " [[0.9613664]\n",
      " [0.9762951]]\n"
     ]
    }
   ],
   "source": [
    "#生成会话，训练steps轮\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "   \n",
    "    #训练模型\n",
    "    STEPS = 3000\n",
    "    for i in range(STEPS):\n",
    "        start = (i*BATCH_SIZE) % 32\n",
    "        end = start + BATCH_SIZE\n",
    "        sess.run(train_step,feed_dict={x:X[start:end],y_: Y_[start:end]})\n",
    "        if i % 500 == 0:\n",
    "            total_loss = sess.run(loss,feed_dict={x: X,y_ : Y_})\n",
    "            print(\"After %d traning step(s),loss on all data is %g\"%(i,total_loss))\n",
    "            print(\"w1:\\n\",sess.run(w1))\n",
    "        \n",
    "    #输出训练后的参数值\n",
    "    print(\" final w1:\\n\",sess.run(w1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
